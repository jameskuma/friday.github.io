<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>
      Zelin (Friday) Gao
    </title>

    <meta name="author" content="Zelin (Friday) Gao">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¥³</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  <!-- your name here -->
                  Zelin (Friday) Gao
                </p>
                <p>
                  <!-- your bio here -->
                  I am a Master (Sep. 2021 - Mar. 2024) student at College of Control Science and Engineering, <a href="https://www.zju.edu.cn">Zhejiang Unerveristy</a>.
                  Before that, I obtatin my B.Eng (Sep. 2017 - Jun. 2021) from College of Instrumentation and Electrical Engineering, <a href="https://www.jlu.edu.cn">Jilin Unerveristy</a>.
                </p>
                <p>
                  <!-- your bio here  -->
                  I was a remote research intern at <a href="https://ccvl.jhu.edu/team">CCVL</a> in <a href="https://www.jhu.edu">Johns Hopkins University</a>, and working with <a href="https://yutongbai.com/">Yutong Bai</a>.
                  <!-- Before that, I was a research intern at <a href="https://shlab.org.cn/pc/home">Shanghai AI Lab</a>, mentored by <a href="https://liuziwei7.github.io/">Prof. Ziwei Liu</a> at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>.  -->
                  I also work very closely with <a href="https://weichnn.github.io/">Prof. Weichen Dai</a> at Hangzhou Dianzi University and <a href="http://elkula.com/">Jiajun Jiang</a> at Alibaba Group.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jamesgzl@zju.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="Bio/cv_en_gzl.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="Bio/research_statement.pdf">Reasearch Statement</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/Kuma_Scofield">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jameskuma/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="picture/gang.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="picture/qiyu.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Recent News</h2>
              <p> [2024.02] Our paper is accepted by CVPR2024! </p>
              <p>
                <!-- [2023.08] <font color="red"><strong>I'm looking for PhD to start in Fall 2024. Please feel free to send emails if you have any leads!</strong></font> -->
                [2024.01] <font color="red"><strong>GAP year starts and look forward to research intern!</strong></font>
              </p>
              <p> [2023.07] Our paper is accepted by ICCV2023! </p>
              <p> [2022.06] Our paper is accepted by IROS2022&RAL! </p>
            </td>
          </tr>
        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                My research interest lies in 3D Vision, especially in the field of Neural Rendering, Implicit Representation, and SLAM.
                I also love 3D/4D Object Generation! Please check my github and see my implementation for Diffusion Model.
                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
          <tr onmouseout="ape_stop()" onmouseover="ape_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='picture/cvpr2024/cvpr2024.png' width="170">
              </div>
              <script type="text/javascript">
                function ape_start() {
                  document.getElementById('ape_image').style.opacity = "1";
                }
    
                function ape_stop() {
                  document.getElementById('ape_image').style.opacity = "0";
                }
                ape_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.pdf">
                <span class="papertitle">SAP3D: The More You See in 2D, the More You Perceive in 3D</span>
              </a>
              <br>
              <strong>Zelin Gao</strong>,
              Xinyang Han,
              <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
              <a href="https://shubham-goel.github.io/">Shubham Goel</a>,
              <a href="https://yossigandelsman.github.io/">Yossi Gandelsman</a>,
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="http://jameskuma.github.io">project page</a>
              /
              <a href="http://jameskuma.github.io">video</a>
              /
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.pdf">paper</a>
              <p></p>
              <p>
                Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. 
                Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. 
              </p>
            </td>
          </tr>

      <tr onmouseout="ape_stop()" onmouseover="ape_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='ape_image'><video  width="190" height="140" muted autoplay loop>
            <source src="picture/iccv2023/ape.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='picture/iccv2023/ape.png' width="170">
          </div>
          <script type="text/javascript">
            function ape_start() {
              document.getElementById('ape_image').style.opacity = "1";
            }

            function ape_stop() {
              document.getElementById('ape_image').style.opacity = "0";
            }
            ape_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.pdf">
            <span class="papertitle">Adaptive Positional Encoding for Bundle-Adjusting Neural Radiance Fields</span>
          </a>
          <br>
          <strong>Zelin Gao</strong>,
          <a href="https://weichnn.github.io/">Weichen Dai</a>,
          <a href="https://person.zju.edu.cn/en/zhangyu">Yu Zhang</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="http://jameskuma.github.io">project page</a>
          /
          <a href="http://jameskuma.github.io">video</a>
          /
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.pdf">paper</a>
          <p></p>
          <p>
          Adaptive Positional Encoding is proposed in this paper to train neural radiance fields from unknwon camera poses (or even initrinics). 
          The theoretical relationship between Positional Encoding and Fourier Series Regression is investigated to prove that learnable frequencies can improve both accuracy of camera parameter estimation and view synthesis quality of scene representation.
          </p>
        </td>
      </tr>

      <tr onmouseout="tslam_stop()" onmouseover="tslam_start()"  >
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='picture/arxiv2023/hg3nerf.png' width=170>
          </div>
          <script type="text/javascript">
            function tslam_start() {
              document.getElementById('tslam_image').style.opacity = "1";
            }

            function tslam_stop() {
              document.getElementById('tslam_image').style.opacity = "0";
            }
            tslam_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jameskuma.github.io">
            <span class="papertitle">HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</span>
          </a>
          <br>
          <strong>Zelin Gao</strong>,
          <a href="https://weichnn.github.io/">Weichen Dai</a>,
          <a href="https://person.zju.edu.cn/en/zhangyu">Yu Zhang</a>
          <br>
          <em>ARXIV</em>, 2023
          <br>
          <a href="http://jameskuma.github.io">project page</a>
          /
          <a href="https://www.bilibili.com/video/BV1T94y1U7Ec">video</a>
          /
          <a href="https://arxiv.org/abs/2401.11711">arxiv</a>
          <p></p>
          <p>
            In this paper, we exploit the geometric, semantic, and photometric guidance to represent the neural radiance fields from sparse view inputs. 
            We propose hierarchical geometric guidance (HGG) to sample volume points with the depth prior and hierarchical semantic guidance (HSG) to supervise semantic consistency of the complex real-world scenarios using CLIP.
          </p>
        </td>
      </tr>

      <tr onmouseout="tslam_stop()" onmouseover="tslam_start()"  >
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='picture/iros2022/tislam.jpg' width="170">
          </div>
          <script type="text/javascript">
            function tslam_start() {
              document.getElementById('tslam_image').style.opacity = "1";
            }

            function tslam_stop() {
              document.getElementById('tslam_image').style.opacity = "0";
            }
            tslam_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jameskuma.github.io">
            <span class="papertitle">Thermal-Inertial SLAM for the Environments with Challenging Illumination</span>
          </a>
          <br>
          <a href="http://elkula.com/">Jiajun Jiang</a>,
          <a href="https://weichnn.github.io/">Weichen Dai</a>,
          Xingxin Chen,
          <strong>Zelin Gao</strong>,
          <a href="https://person.zju.edu.cn/en/zhangyu">Yu Zhang</a>
          <br>
          <em>RAL&IROS</em>, 2022 <font color="red"><strong>(Oral Presentation)</strong></font>
          <br>
          <a href="http://jameskuma.github.io">project page</a>
          /
          <a href="https://www.bilibili.com/video/BV1T94y1U7Ec">video</a>
          /
          <a href="https://ieeexplore.ieee.org/document/9804793">ieee</a>
          <p></p>
          <p>
          Thermal images are used in this paper to realize a robust visual SLAM system in challenging environment. 
          The proposed method, a thermal-inertial SLAM system, represents several improvements, including SVD-based image processing and Thermal-RAFT tracking methods.
          </p>
        </td>
      </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  ðŸ˜‚ This page is stolen from  <a href="https://jonbarron.info/"">Jon Barron</a>. ðŸ˜‚
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
